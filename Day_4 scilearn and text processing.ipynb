{
 "metadata": {
  "name": "",
  "signature": "sha256:3e6b7926dbf7c990f03379526c8236cb670aab77b9763d39038375a37e388fcc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "measurements = [{'city': 'Dubai', 'temperature': 33.},\n",
      "{'city': 'London', 'temperature': 12.},\n",
      "{'city': 'San Fransisco', 'temperature': 18.},]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction import DictVectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec = DictVectorizer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A way of encoding the feature from the label\n",
      "# peform a dot product < x_i, y_p > where x_i is the ith row and y_p is the pth (last) column to get the feature of the ith\n",
      "# label\n",
      "vec.fit_transform(measurements).toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "array([[  1.,   0.,   0.,  33.],\n",
        "       [  0.,   1.,   0.,  12.],\n",
        "       [  0.,   0.,   1.,  18.]])"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get back the labels\n",
      "vec.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Implements both tokenization and occurrence counting in a single class\n",
      "from sklearn.feature_extraction.text import CountVectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd \n",
      "\n",
      "questions_all = pd.read_csv('tagged_data.csv', index_col=0)\n",
      "questions_all = questions_all[ ['question', 'tags'] ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>question</th>\n",
        "      <th>tags</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>index</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> Given the following table: EOT  TEXT( &amp;beginta...</td>\n",
        "      <td>                       ['calculus', 'derivatives']</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> Let \\(f(x)=\\frac{$b}{x- $a}\\). Then according ...</td>\n",
        "      <td>                       ['calculus', 'derivatives']</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> If \\( f(x) = $a1 x + $b1  \\), find \\( f'( $x1 ...</td>\n",
        "      <td>       ['calculus', 'derivatives', 'tangent line']</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>  Find \\(a\\) and \\(b\\) such that the function  ...</td>\n",
        "      <td> ['calculus', 'derivatives', 'differentiable fu...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>  Graphs A and B are approximate graphs of \\( f...</td>\n",
        "      <td>                       ['calculus', 'derivatives']</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "                                                question  \\\n",
        "index                                                      \n",
        "0      Given the following table: EOT  TEXT( &beginta...   \n",
        "1      Let \\(f(x)=\\frac{$b}{x- $a}\\). Then according ...   \n",
        "2      If \\( f(x) = $a1 x + $b1  \\), find \\( f'( $x1 ...   \n",
        "3       Find \\(a\\) and \\(b\\) such that the function  ...   \n",
        "4       Graphs A and B are approximate graphs of \\( f...   \n",
        "\n",
        "                                                    tags  \n",
        "index                                                     \n",
        "0                            ['calculus', 'derivatives']  \n",
        "1                            ['calculus', 'derivatives']  \n",
        "2            ['calculus', 'derivatives', 'tangent line']  \n",
        "3      ['calculus', 'derivatives', 'differentiable fu...  \n",
        "4                            ['calculus', 'derivatives']  "
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = []\n",
      "\n",
      "for i in range(5):\n",
      "    corpus.append(questions_all.iloc[i]['question'])\n",
      "    \n",
      "corpus = np.array(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import ast\n",
      "vocab = []\n",
      "\n",
      "for i in range(5):\n",
      "    vocab.append(ast.literal_eval(questions_all.iloc[i]['tags']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(vocabulary = )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "[['calculus', 'derivatives'],\n",
        " ['calculus', 'derivatives'],\n",
        " ['calculus', 'derivatives', 'tangent line'],\n",
        " ['calculus', 'derivatives', 'differentiable function'],\n",
        " ['calculus', 'derivatives']]"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Vectorize the corpus of text\n",
      "# Count the number of occurrences of words in the corpus\n",
      "X = vectorizer.fit_transform(corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# matrix representing the frequency of each term in each question\n",
      "# 5 rows correspong the the 5 questions\n",
      "X.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "array([[2, 0, 0, 0, 0, 1, 2, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
        "        0, 1, 1, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
        "        0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n",
        "       [1, 0, 0, 0, 0, 1, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 2, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 1, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0],\n",
        "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
        "       [0, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 2, 1, 1, 0, 0, 2, 0, 2, 0, 0,\n",
        "        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0,\n",
        "        2, 0, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 0, 0, 0, 1, 2, 1, 1, 0],\n",
        "       [1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]])"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# exlicitly see the output of the tokenizer on some string\n",
      "# here I just write some random math problem and see what is spits back\n",
      "analyze = vectorizer.build_analyzer()\n",
      "analyze(\"What is the limit of \\( x^2 \\) as \\( x \\) goes to \\( \\infty \\)?\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "[u'what', u'is', u'the', u'limit', u'of', u'as', u'goes', u'to', u'infty']"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get back all of the features from the fitted data i.e. the 5 questions in the corpus\n",
      "# just print 5 of them so it doesn't kill me screen\n",
      "vectorizer.get_feature_names()[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "[u'10', u'30', u'a1', u'aa', u'an']"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.vocabulary_.get('limit')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "41"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# explicitly vectorize a new string see what is spits out.\n",
      "# this has nothing to do with math so it should be almost empty\n",
      "vectorizer.transform([\"don't say that\"]).toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# instead of only looking at occurences of single words, lets also look at 2-grams\n",
      "\n",
      "bigram_vectorizer = CountVectorizer(ngram_range=(1,2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
      "analyze = bigram_vectorizer.build_analyzer()\n",
      "analyze(\"What is the limit of \\( x^2 \\) as \\( x \\) goes to \\( \\infty \\)?\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "[u'what',\n",
        " u'is',\n",
        " u'the',\n",
        " u'limit',\n",
        " u'of',\n",
        " u'x',\n",
        " u'2',\n",
        " u'as',\n",
        " u'x',\n",
        " u'goes',\n",
        " u'to',\n",
        " u'infty',\n",
        " u'what is',\n",
        " u'is the',\n",
        " u'the limit',\n",
        " u'limit of',\n",
        " u'of x',\n",
        " u'x 2',\n",
        " u'2 as',\n",
        " u'as x',\n",
        " u'x goes',\n",
        " u'goes to',\n",
        " u'to infty']"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create another array that is fit with the 2-grams as well\n",
      "X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
      "X_2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "array([[0, 0, 0, ..., 0, 0, 0],\n",
        "       [0, 0, 0, ..., 0, 0, 0],\n",
        "       [0, 0, 0, ..., 1, 1, 1],\n",
        "       [0, 0, 1, ..., 0, 0, 0],\n",
        "       [1, 1, 0, ..., 0, 0, 0]])"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "\n",
      "transformer = TfidfTransformer()\n",
      "\n",
      "transformer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "TfidfTransformer(norm=u'l2', smooth_idf=True, sublinear_tf=False,\n",
        "         use_idf=True)"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# freq counts where first term always present hence less interested \n",
      "# while the other 2 present less than half the time hence\n",
      "# probably more interesting\n",
      "counts = [[3,0,1],\n",
      "         [2,0,0],\n",
      "         [3,0,0],\n",
      "         [4,0,0],\n",
      "         [3,2,0],\n",
      "         [3,0,2]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# tfidf term frequency inverse document frequency\n",
      "tfidf = transformer.fit_transform(counts)\n",
      "tfidf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 50,
       "text": [
        "<6x3 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 9 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# each row normalized to have unit euclidean norm\n",
      "tfidf.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 52,
       "text": [
        "array([[ 0.85151335,  0.        ,  0.52433293],\n",
        "       [ 1.        ,  0.        ,  0.        ],\n",
        "       [ 1.        ,  0.        ,  0.        ],\n",
        "       [ 1.        ,  0.        ,  0.        ],\n",
        "       [ 0.55422893,  0.83236428,  0.        ],\n",
        "       [ 0.63035731,  0.        ,  0.77630514]])"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the weights of each feature computed by the fit method\n",
      "transformer.idf_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "array([ 1.        ,  2.25276297,  1.84729786])"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "# Vectorize and perform tfidf in one operation\n",
      "# Also, work on 1-gram and 2-gram\n",
      "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=1)\n",
      "tfidf = vectorizer.fit_transform(corpus)\n",
      "tfidf\n",
      "#tfidf.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 70,
       "text": [
        "<5x161 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 182 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Also, using stop words\n",
      "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=1, stop_words='english')\n",
      "tfidf = vectorizer.fit_transform(corpus)\n",
      "tfidf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 71,
       "text": [
        "<5x124 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 136 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Using the english stop words removed 37 words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 73,
       "text": [
        "array([[ 0.21407561,  0.1598267 ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.15231656,  0.21407561,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.31965341,  0.1598267 ,  0.1598267 ,\n",
        "         0.1598267 ,  0.1598267 ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.1598267 ,  0.1598267 ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.1598267 ,  0.1598267 ,\n",
        "         0.        ,  0.        ,  0.1598267 ,  0.1598267 ,  0.1598267 ,\n",
        "         0.1598267 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.12894718,  0.1598267 ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.1598267 ,  0.1598267 ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.1598267 ,  0.1598267 ,  0.1598267 ,  0.1598267 ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.1598267 ,\n",
        "         0.1598267 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.31965341,  0.1598267 ,\n",
        "         0.1598267 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.1598267 ,  0.1598267 ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.10121528,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.36007751,  0.10121528,  0.        ,\n",
        "         0.15113262,  0.15113262,  0.15113262,  0.15113262,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.15113262,\n",
        "         0.15113262,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.24386569,  0.        ,  0.15113262,  0.15113262,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.15113262,  0.15113262,\n",
        "         0.        ,  0.        ,  0.24386569,  0.15113262,  0.15113262,\n",
        "         0.        ,  0.        ,  0.        ,  0.45339785,  0.30226523,\n",
        "         0.15113262,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.15113262,\n",
        "         0.15113262,  0.15113262,  0.15113262,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.15113262,\n",
        "         0.15113262,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.26511998,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.12633109,  0.        ,  0.26511998,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.26511998,  0.26511998,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.26511998,  0.26511998,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.26511998,  0.26511998,  0.26511998,  0.26511998,  0.        ,\n",
        "         0.        ,  0.26511998,  0.26511998,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.26511998,  0.26511998],\n",
        "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.10231272,  0.10231272,  0.09750512,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.10231272,\n",
        "         0.10231272,  0.10231272,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.20462544,  0.10231272,  0.10231272,\n",
        "         0.20462544,  0.10231272,  0.10231272,  0.10231272,  0.10231272,\n",
        "         0.10231272,  0.10231272,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.20462544,  0.20462544,  0.        ,  0.        ,\n",
        "         0.20462544,  0.20462544,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.10231272,  0.10231272,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.10231272,\n",
        "         0.10231272,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.10231272,  0.10231272,  0.33018103,  0.        ,  0.        ,\n",
        "         0.10231272,  0.10231272,  0.20462544,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.16509051,  0.20462544,  0.        ,\n",
        "         0.        ,  0.        ,  0.10231272,  0.10231272,  0.        ,\n",
        "         0.        ,  0.20462544,  0.10231272,  0.10231272,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.10231272,  0.10231272,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.20462544,  0.20462544,  0.        ,\n",
        "         0.        ,  0.16509051,  0.        ,  0.10231272,  0.10231272,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.10231272,\n",
        "         0.10231272,  0.        ,  0.        ,  0.10231272,  0.10231272,\n",
        "         0.10231272,  0.10231272,  0.        ,  0.        ],\n",
        "       [ 0.20741049,  0.        ,  0.        ,  0.30970119,  0.30970119,\n",
        "         0.        ,  0.        ,  0.14757428,  0.20741049,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.30970119,  0.30970119,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.24986497,  0.        ,  0.30970119,\n",
        "         0.30970119,  0.30970119,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.24986497,  0.30970119,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
        "         0.        ,  0.        ,  0.        ,  0.        ]])"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}