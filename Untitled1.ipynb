{
 "metadata": {
  "name": "",
  "signature": "sha256:ab5a7f83b95339733af6864b6c5c7b7c6c427dcfb1a5b66b6e167e6d94748181"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd \n",
      "from time import time\n",
      "import ast # Convert string expressions to python objects"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load all question data into memory\n",
      "questions_all = pd.read_csv('tagged_data.csv', index_col=0)\n",
      "\n",
      "print 'Loaded ' + str(questions_all.shape[0]) + ' tagged questions into memory'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loaded 19612 tagged questions into memory\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "calculus = []\n",
      "\n",
      "t0 = time()\n",
      "\n",
      "n = 1000 #questions_all.shape[0]\n",
      "\n",
      "for i in range(n):\n",
      "    if i != 376:             # some fucking stupid nan\n",
      "        row = questions_all.iloc[i]\n",
      "        tag_string = questions_all['tags'].iloc[i]\n",
      "        try:\n",
      "            tag_list = ast.literal_eval(tag_string)\n",
      "            if 'calculus' in tag_list:\n",
      "                calculus.append(row)\n",
      "        except ValueError:\n",
      "            print tag_string\n",
      "    \n",
      "t1 = time()\n",
      "\n",
      "print 'Took ' + str(t1-t0) + ' seconds to loop through list'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Took 0.533413887024 seconds to loop through list\n"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#calculus is a pandas df containing all problems with a calculus tag\n",
      "calculus = pd.DataFrame(calculus)\n",
      "print 'There are ' + str(calculus.shape[0]) + ' calculus problems'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 330 calculus problems\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "questions = calculus['question'].values\n",
      "tags = calculus['tags']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get some test data\n",
      "test_data = []\n",
      "\n",
      "n_samples = 1000\n",
      "\n",
      "for i in range(n_samples):\n",
      "    if i != 376:\n",
      "        test_data.append(questions_all.iloc[i])\n",
      " \n",
      "test_data = pd.DataFrame(test_data)\n",
      "test_data = test_data['question'].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_features = 1000\n",
      "n_topics = 10\n",
      "n_top_words = 20\n",
      "\n",
      "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate the tfidf from the data\n",
      "tfidf = vectorizer.fit_transform(test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nmf = NMF(n_components=n_topics, random_state=1).fit(tfidf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_names = vectorizer.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for topic_idx, topic in enumerate(nmf.components_):\n",
      "    print(\"Topic #%d:\" % topic_idx)\n",
      "    #print topic.argsort()\n",
      "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
      "    print()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Topic #0:\n",
        "dx displaystyle integral evaluate int indefinite int_ frac definite 35 par ans_rule dy dt 25 sqrt 45 30 right integration\n",
        "()\n",
        "Topic #1:\n",
        "answer write interval infinity infty bitalic eitalic intervals separated includes notation union symbol inequality needed solve 35 enter ebold bbold\n",
        "()\n",
        "Topic #2:\n",
        "limit lim_ rightarrow exist does dne 25 frac enter evaluate par a1 ans_rule sqrt displaystyle infty b1 e1 a2 limits\n",
        "()\n",
        "Topic #3:\n",
        "eot ans ev2 text num_cmp answers asymptotes local 20 inf use following interval_cmp enter values strings concave minima maxima vertical\n",
        "()\n",
        "Topic #4:\n",
        "10 ans_rule constants lim_ displaystyle point maximum array minimum rightarrow bitalic eitalic demand following absolute value cr center circle elasticity\n",
        "()\n",
        "Topic #5:\n",
        "30 suppose ln ans_rule par frac 20 p1 dy displaystyle dt p2 partial sqrt line dx tangent xp slope cost\n",
        "()\n",
        "Topic #6:\n",
        "mathbf left right rangle langle leq ans_rule vector le 20 field theta partial sin par nabla compute 15 curve array\n",
        "()\n",
        "Topic #7:\n",
        "par 40 ans_rule function use values graph local let list real note answers enter ebold bbold print_q mc indicate volume\n",
        "()\n",
        "Topic #8:\n",
        "dollar 20 percent years compounded monthly ans_rule answer rate months b1 month simple account invested year frac payment b2 ira\n",
        "()\n",
        "Topic #9:\n",
        "x0 tangent line y0 equation point ans_rule 20 plane given graph surface z0 slope 60 curve values langle rangle horizontal\n",
        "()\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Topic modelling using NMF\n",
      "# Adapted from http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html\n",
      "\n",
      "from time import time\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.decomposition import NMF\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "\n",
      "n_samples = 20000\n",
      "n_features = 1000\n",
      "n_topics = 10\n",
      "n_top_words = 20\n",
      "\n",
      "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
      "# to filter out useless terms early on: the posts are stripped of headers,\n",
      "# footers and quoted replies, and common English words, words occurring in\n",
      "# only one document or in at least 95% of the documents are removed.\n",
      "\n",
      "t0 = time()\n",
      "print(\"Loading dataset and extracting TF-IDF features...\")\n",
      "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
      "                             remove=('headers', 'footers', 'quotes'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading dataset and extracting TF-IDF features...\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print dataset.data[0]\n",
      "label_id = dataset.target[0]\n",
      "print 'Label: ' + dataset.target_names[label_id]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Well i'm not sure about the story nad it did seem biased. What\n",
        "I disagree with is your statement that the U.S. Media is out to\n",
        "ruin Israels reputation. That is rediculous. The U.S. media is\n",
        "the most pro-israeli media in the world. Having lived in Europe\n",
        "I realize that incidences such as the one described in the\n",
        "letter have occured. The U.S. media as a whole seem to try to\n",
        "ignore them. The U.S. is subsidizing Israels existance and the\n",
        "Europeans are not (at least not to the same degree). So I think\n",
        "that might be a reason they report more clearly on the\n",
        "atrocities.\n",
        "\tWhat is a shame is that in Austria, daily reports of\n",
        "the inhuman acts commited by Israeli soldiers and the blessing\n",
        "received from the Government makes some of the Holocaust guilt\n",
        "go away. After all, look how the Jews are treating other races\n",
        "when they got power. It is unfortunate.\n",
        "\n",
        "Label: talk.politics.mideast\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t0 = time()\n",
      "\n",
      "#Dont allow featurs that occur in over 95% of documents or that only appear in one document, plus common words\n",
      "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english')\n",
      "\n",
      "# Calculate the tfidf from the data\n",
      "tfidf = vectorizer.fit_transform(dataset.data[:n_samples])\n",
      "\n",
      "print(\"done in %0.3fs.\" % (time() - t0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "done in 3.183s.\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t0 = time()\n",
      "\n",
      "# Fit the NMF model\n",
      "print(\"Fitting the NMF model with n_samples=%d and n_features=%d...\" % (n_samples, n_features))\n",
      "nmf = NMF(n_components=n_topics, random_state=1).fit(tfidf)\n",
      "print(\"done in %0.3fs.\" % (time() - t0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting the NMF model with n_samples=20000 and n_features=1000...\n",
        "done in 16.823s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/sklearn/decomposition/nmf.py:533: UserWarning: Iteration limit reached during fit\n",
        "  warnings.warn(\"Iteration limit reached during fit\")\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "feature_names = vectorizer.get_feature_names()\n",
      "\n",
      "print feature_names\n",
      "\n",
      "\"\"\"\n",
      "for topic_idx, topic in enumerate(nmf.components_):\n",
      "    print(\"Topic #%d:\" % topic_idx)\n",
      "    #print topic.argsort()\n",
      "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
      "    print()\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'00', u'000', u'01', u'02', u'03', u'04', u'0d', u'0t', u'10', u'100', u'11', u'12', u'128', u'13', u'14', u'145', u'15', u'16', u'17', u'18', u'19', u'1990', u'1991', u'1992', u'1993', u'1d9', u'1st', u'1t', u'20', u'200', u'21', u'22', u'23', u'24', u'25', u'250', u'26', u'27', u'28', u'29', u'2di', u'2tm', u'30', u'300', u'31', u'32', u'33', u'34', u'34u', u'35', u'36', u'37', u'38', u'39', u'3d', u'3t', u'40', u'42', u'43', u'44', u'45', u'50', u'500', u'55', u'60', u'64', u'6ei', u'70', u'75', u'75u', u'7ey', u'7u', u'80', u'800', u'86', u'90', u'91', u'92', u'93', u'9v', u'a86', u'able', u'ac', u'accept', u'access', u'according', u'act', u'action', u'actually', u'add', u'addition', u'address', u'administration', u'advance', u'age', u'ago', u'agree', u'ah', u'air', u'al', u'algorithm', u'allow', u'allowed', u'alt', u'america', u'american', u'analysis', u'anonymous', u'answer', u'answers', u'anti', u'anybody', u'apparently', u'appears', u'apple', u'application', u'applications', u'appreciate', u'appreciated', u'approach', u'appropriate', u'apr', u'april', u'archive', u'area', u'aren', u'argument', u'armenia', u'armenian', u'armenians', u'arms', u'army', u'article', u'articles', u'ask', u'asked', u'asking', u'assume', u'atheism', u'attack', u'au', u'author', u'authority', u'available', u'average', u'avoid', u'away', u'ax', u'b8f', u'bad', u'base', u'based', u'basic', u'basis', u'belief', u'believe', u'best', u'better', u'bh', u'bhj', u'bible', u'big', u'bike', u'bit', u'bits', u'black', u'block', u'blood', u'board', u'body', u'book', u'books', u'bought', u'box', u'break', u'bring', u'brought', u'btw', u'build', u'building', u'built', u'bus', u'business', u'buy', u'bxn', u'ca', u'cable', u'california', u'called', u'calls', u'came', u'canada', u'car', u'card', u'cards', u'care', u'carry', u'cars', u'case', u'cases', u'cause', u'cd', u'center', u'certain', u'certainly', u'chance', u'change', u'changed', u'changes', u'check', u'chicago', u'child', u'children', u'chip', u'chips', u'choice', u'christ', u'christian', u'christianity', u'christians', u'church', u'chz', u'citizens', u'city', u'claim', u'claims', u'class', u'clear', u'clearly', u'clinton', u'clipper', u'close', u'code', u'color', u'com', u'come', u'comes', u'coming', u'command', u'comments', u'commercial', u'common', u'communications', u'community', u'comp', u'company', u'complete', u'completely', u'computer', u'condition', u'conference', u'congress', u'consider', u'considered', u'contact', u'contains', u'context', u'continue', u'control', u'controller', u'copy', u'correct', u'cost', u'couldn', u'country', u'couple', u'course', u'court', u'cover', u'create', u'created', u'crime', u'cross', u'cs', u'current', u'currently', u'cut', u'cx', u'd9', u'data', u'date', u'dave', u'david', u'day', u'days', u'db', u'dc', u'dead', u'deal', u'death', u'decided', u'defense', u'define', u'deleted', u'department', u'des', u'design', u'designed', u'details', u'development', u'device', u'devices', u'did', u'didn', u'die', u'difference', u'different', u'difficult', u'digital', u'directly', u'directory', u'discussion', u'disk', u'display', u'distribution', u'division', u'dod', u'does', u'doesn', u'doing', u'don', u'dos', u'doubt', u'dr', u'drive', u'driver', u'drivers', u'drives', u'drug', u'early', u'earth', u'easily', u'east', u'easy', u'ed', u'edu', u'effect', u'electronic', u'email', u'encryption', u'end', u'enforcement', u'engine', u'entire', u'entry', u'environment', u'error', u'escrow', u'especially', u'event', u'events', u'evidence', u'exactly', u'example', u'excellent', u'exist', u'existence', u'exists', u'expect', u'experience', u'explain', u'export', u'extra', u'face', u'fact', u'faith', u'family', u'faq', u'far', u'fast', u'faster', u'father', u'fax', u'fbi', u'federal', u'feel', u'field', u'figure', u'file', u'files', u'final', u'finally', u'fine', u'firearms', u'floppy', u'folks', u'follow', u'following', u'food', u'force', u'form', u'format', u'free', u'freedom', u'friend', u'ftp', u'function', u'functions', u'future', u'g9v', u'game', u'games', u'gas', u'gave', u'general', u'generally', u'gets', u'getting', u'gif', u'given', u'gives', u'giving', u'giz', u'gk', u'gm', u'goal', u'god', u'goes', u'going', u'good', u'got', u'gov', u'government', u'graphics', u'great', u'greek', u'ground', u'group', u'groups', u'guess', u'gun', u'guns', u'guy', u'half', u'hand', u'happen', u'happened', u'happens', u'happy', u'hard', u'hardware', u'haven', u'having', u'head', u'health', u'hear', u'heard', u'held', u'hell', u'help', u'hi', u'high', u'higher', u'history', u'hit', u'hockey', u'hold', u'home', u'hope', u'hot', u'hours', u'house', u'hp', u'human', u'hz', u'ibm', u'ide', u'idea', u'ideas', u'ii', u'image', u'images', u'imagine', u'important', u'include', u'included', u'includes', u'including', u'individual', u'info', u'information', u'input', u'inside', u'installed', u'instead', u'insurance', u'int', u'interested', u'interesting', u'interface', u'internal', u'international', u'internet', u'involved', u'isn', u'israel', u'israeli', u'issue', u'issues', u'jesus', u'jewish', u'jews', u'jim', u'job', u'jobs', u'john', u'jpeg', u'just', u'key', u'keyboard', u'keys', u'kill', u'killed', u'kind', u'knew', u'know', u'knowledge', u'known', u'knows', u'la', u'land', u'language', u'large', u'late', u'later', u'launch', u'law', u'laws', u'league', u'learn', u'leave', u'left', u'legal', u'let', u'letter', u'level', u'library', u'life', u'light', u'like', u'likely', u'limited', u'line', u'lines', u'list', u'little', u'live', u'living', u'lk', u'll', u'local', u'long', u'longer', u'look', u'looked', u'looking', u'looks', u'lord', u'lost', u'lot', u'lots', u'love', u'low', u'lower', u'ma', u'mac', u'machine', u'machines', u'mail', u'main', u'major', u'make', u'makes', u'making', u'man', u'manager', u'manual', u'mark', u'market', u'mass', u'material', u'matter', u'max', u'maybe', u'mb', u'mean', u'meaning', u'means', u'media', u'medical', u'members', u'memory', u'men', u'mention', u'mentioned', u'message', u'mike', u'military', u'million', u'mind', u'mit', u'mode', u'model', u'modem', u'money', u'monitor', u'month', u'months', u'moral', u'motif', u'mouse', u'mr', u'ms', u'multiple', u'nasa', u'national', u'nature', u'near', u'necessary', u'need', u'needed', u'needs', u'net', u'network', u'new', u'news', u'newsgroup', u'nhl', u'nice', u'night', u'non', u'normal', u'north', u'note', u'nsa', u'number', u'numbers', u'object', u'obvious', u'obviously', u'offer', u'office', u'official', u'oh', u'ok', u'old', u'ones', u'open', u'opinion', u'opinions', u'orbit', u'order', u'org', u'organization', u'original', u'os', u'output', u'outside', u'package', u'page', u'paper', u'particular', u'parts', u'party', u'past', u'paul', u'pay', u'pc', u'peace', u'people', u'perfect', u'performance', u'period', u'person', u'personal', u'phone', u'physical', u'pick', u'picture', u'pin', u'pittsburgh', u'pl', u'place', u'places', u'plan', u'play', u'player', u'players', u'plus', u'point', u'points', u'police', u'policy', u'political', u'population', u'port', u'position', u'possible', u'possibly', u'post', u'posted', u'posting', u'power', u'pp', u'present', u'president', u'press', u'pretty', u'previous', u'price', u'printer', u'privacy', u'private', u'pro', u'probably', u'problem', u'problems', u'process', u'product', u'products', u'program', u'programs', u'project', u'protect', u'prove', u'provide', u'provided', u'provides', u'pts', u'pub', u'public', u'published', u'purpose', u'quality', u'question', u'questions', u'quite', u'radio', u'ram', u'range', u'rate', u'read', u'reading', u'real', u'really', u'reason', u'reasonable', u'reasons', u'received', u'recent', u'recently', u'record', u'red', u'reference', u'related', u'release', u'religion', u'religious', u'remember', u'reply', u'report', u'reported', u'reports', u'request', u'require', u'required', u'requires', u'research', u'response', u'rest', u'result', u'results', u'return', u'right', u'rights', u'road', u'rom', u'room', u'rules', u'run', u'running', u'runs', u'russian', u'safety', u'said', u'sale', u'san', u'satellite', u'save', u'saw', u'say', u'saying', u'says', u'school', u'sci', u'science', u'scientific', u'screen', u'scsi', u'season', u'second', u'secret', u'section', u'secure', u'security', u'seen', u'self', u'sell', u'send', u'sense', u'sent', u'serial', u'series', u'server', u'service', u'set', u'shall', u'shipping', u'short', u'shot', u'shuttle', u'similar', u'simple', u'simply', u'sin', u'single', u'site', u'sites', u'situation', u'size', u'sl', u'small', u'society', u'software', u'solution', u'son', u'soon', u'sorry', u'sort', u'sound', u'sounds', u'source', u'sources', u'south', u'soviet', u'space', u'special', u'specific', u'speed', u'st', u'standard', u'start', u'started', u'state', u'statement', u'states', u'station', u'stephanopoulos', u'steve', u'stop', u'story', u'street', u'strong', u'study', u'stuff', u'subject', u'suggest', u'sun', u'supply', u'support', u'supports', u'supposed', u'sure', u'switch', u'systems', u'taken', u'takes', u'taking', u'talk', u'talking', u'tape', u'tar', u'tax', u'team', u'teams', u'technical', u'technology', u'tell', u'term', u'terms', u'test', u'text', u'thank', u'thanks', u'theory', u'thing', u'things', u'think', u'thinking', u'thought', u'time', u'times', u'title', u'today', u'told', u'took', u'tools', u'total', u'trade', u'transfer', u'tried', u'trouble', u'true', u'truth', u'try', u'trying', u'turkey', u'turkish', u'turks', u'turn', u'tv', u'type', u'uk', u'understand', u'unfortunately', u'unit', u'united', u'university', u'unix', u'unless', u'usa', u'use', u'used', u'useful', u'usenet', u'user', u'users', u'uses', u'using', u'usually', u'value', u'values', u'van', u'various', u've', u'version', u'vga', u'video', u'view', u'voice', u'volume', u'vs', u'w7', u'wait', u'want', u'wanted', u'wants', u'war', u'washington', u'wasn', u'water', u'way', u'ways', u'weapons', u'week', u'weeks', u'went', u'white', u'wide', u'widget', u'willing', u'win', u'window', u'windows', u'wire', u'wish', u'wm', u'women', u'won', u'word', u'words', u'work', u'worked', u'working', u'works', u'world', u'worth', u'wouldn', u'write', u'writing', u'written', u'wrong', u'wrote', u'x11', u'xt', u'year', u'years', u'yes', u'york', u'young']\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "'\\nfor topic_idx, topic in enumerate(nmf.components_):\\n    print(\"Topic #%d:\" % topic_idx)\\n    #print topic.argsort()\\n    print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\\n    print()\\n'"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}